{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install -q ultralytics"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "djsnh4wQCd6K",
        "outputId": "f4c1bd07-0649-493d-d6ff-288758c87511"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.8/41.8 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m872.3/872.3 kB\u001b[0m \u001b[31m15.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Instance Segmentation with object tracking"
      ],
      "metadata": {
        "id": "GlS3NEyuFSDA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n"
      ],
      "metadata": {
        "id": "j6Op7aXxVyd3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(os.getenv('CUDA_LAUNCH_BLOCKING'))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ILlwF4VOV-8z",
        "outputId": "58f1d67a-6127-44d4-c67b-78538837fbae"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CW_e-wQdB6Te",
        "outputId": "afbe52fc-7828-478d-bfdb-97cf293963d3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "0: 384x640 22 persons, 70.5ms\n",
            "Speed: 3.1ms preprocess, 70.5ms inference, 2.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "tensor([[1479.,  318.],\n",
            "        [1479.,  330.],\n",
            "        [1470.,  339.],\n",
            "        [1467.,  339.],\n",
            "        [1467.,  342.],\n",
            "        [1461.,  348.],\n",
            "        [1458.,  348.],\n",
            "        [1455.,  351.],\n",
            "        [1446.,  351.],\n",
            "        [1446.,  405.],\n",
            "        [1455.,  405.],\n",
            "        [1461.,  411.],\n",
            "        [1461.,  414.],\n",
            "        [1458.,  417.],\n",
            "        [1458.,  420.],\n",
            "        [1455.,  423.],\n",
            "        [1446.,  423.],\n",
            "        [1446.,  441.],\n",
            "        [1458.,  441.],\n",
            "        [1461.,  444.],\n",
            "        [1461.,  453.],\n",
            "        [1464.,  456.],\n",
            "        [1464.,  474.],\n",
            "        [1470.,  480.],\n",
            "        [1479.,  480.],\n",
            "        [1485.,  474.],\n",
            "        [1485.,  471.],\n",
            "        [1482.,  468.],\n",
            "        [1482.,  459.],\n",
            "        [1479.,  456.],\n",
            "        [1479.,  441.],\n",
            "        [1476.,  438.],\n",
            "        [1476.,  435.],\n",
            "        [1479.,  432.],\n",
            "        [1479.,  423.],\n",
            "        [1485.,  417.],\n",
            "        [1485.,  414.],\n",
            "        [1488.,  411.],\n",
            "        [1488.,  408.],\n",
            "        [1494.,  402.],\n",
            "        [1494.,  396.],\n",
            "        [1497.,  393.],\n",
            "        [1497.,  390.],\n",
            "        [1509.,  378.],\n",
            "        [1509.,  372.],\n",
            "        [1512.,  369.],\n",
            "        [1512.,  363.],\n",
            "        [1509.,  360.],\n",
            "        [1509.,  351.],\n",
            "        [1500.,  342.],\n",
            "        [1500.,  336.],\n",
            "        [1503.,  333.],\n",
            "        [1503.,  318.]])\n",
            "tensor([[1182.,  954.],\n",
            "        [1182.,  972.],\n",
            "        [1179.,  975.],\n",
            "        [1179.,  978.],\n",
            "        [1170.,  987.],\n",
            "        [1167.,  987.],\n",
            "        [1164.,  990.],\n",
            "        [1161.,  990.],\n",
            "        [1158.,  993.],\n",
            "        [1146.,  993.],\n",
            "        [1146., 1023.],\n",
            "        [1155., 1023.],\n",
            "        [1158., 1026.],\n",
            "        [1155., 1029.],\n",
            "        [1146., 1029.],\n",
            "        [1146., 1038.],\n",
            "        [1155., 1038.],\n",
            "        [1161., 1044.],\n",
            "        [1161., 1053.],\n",
            "        [1164., 1056.],\n",
            "        [1164., 1080.],\n",
            "        [1218., 1080.],\n",
            "        [1218., 1071.],\n",
            "        [1224., 1065.],\n",
            "        [1227., 1065.],\n",
            "        [1233., 1059.],\n",
            "        [1233., 1056.],\n",
            "        [1242., 1047.],\n",
            "        [1251., 1047.],\n",
            "        [1251., 1026.],\n",
            "        [1239., 1026.],\n",
            "        [1236., 1023.],\n",
            "        [1236., 1020.],\n",
            "        [1233., 1017.],\n",
            "        [1233., 1005.],\n",
            "        [1230., 1002.],\n",
            "        [1230.,  999.],\n",
            "        [1227.,  996.],\n",
            "        [1224.,  996.],\n",
            "        [1218.,  990.],\n",
            "        [1218.,  987.],\n",
            "        [1215.,  984.],\n",
            "        [1215.,  975.],\n",
            "        [1212.,  972.],\n",
            "        [1212.,  954.]])\n",
            "tensor([[693., 654.],\n",
            "        [693., 669.],\n",
            "        [678., 684.],\n",
            "        [666., 684.],\n",
            "        [666., 771.],\n",
            "        [681., 771.],\n",
            "        [684., 774.],\n",
            "        [687., 774.],\n",
            "        [690., 777.],\n",
            "        [690., 786.],\n",
            "        [693., 789.],\n",
            "        [693., 819.],\n",
            "        [696., 822.],\n",
            "        [696., 846.],\n",
            "        [699., 849.],\n",
            "        [714., 849.],\n",
            "        [717., 846.],\n",
            "        [717., 807.],\n",
            "        [723., 801.],\n",
            "        [726., 801.],\n",
            "        [729., 798.],\n",
            "        [729., 795.],\n",
            "        [732., 792.],\n",
            "        [732., 750.],\n",
            "        [738., 744.],\n",
            "        [747., 744.],\n",
            "        [747., 699.],\n",
            "        [738., 699.],\n",
            "        [735., 696.],\n",
            "        [735., 693.],\n",
            "        [732., 690.],\n",
            "        [732., 687.],\n",
            "        [723., 678.],\n",
            "        [723., 675.],\n",
            "        [720., 672.],\n",
            "        [720., 654.]])\n",
            "tensor([[1479.,  702.],\n",
            "        [1479.,  717.],\n",
            "        [1476.,  720.],\n",
            "        [1476.,  723.],\n",
            "        [1470.,  729.],\n",
            "        [1467.,  729.],\n",
            "        [1464.,  732.],\n",
            "        [1458.,  732.],\n",
            "        [1455.,  735.],\n",
            "        [1455.,  744.],\n",
            "        [1452.,  747.],\n",
            "        [1452.,  801.],\n",
            "        [1449.,  804.],\n",
            "        [1449.,  807.],\n",
            "        [1446.,  810.],\n",
            "        [1446.,  813.],\n",
            "        [1443.,  816.],\n",
            "        [1443.,  825.],\n",
            "        [1440.,  828.],\n",
            "        [1440.,  837.],\n",
            "        [1437.,  840.],\n",
            "        [1437.,  846.],\n",
            "        [1434.,  849.],\n",
            "        [1434.,  852.],\n",
            "        [1422.,  864.],\n",
            "        [1410.,  864.],\n",
            "        [1410.,  885.],\n",
            "        [1422.,  885.],\n",
            "        [1425.,  888.],\n",
            "        [1437.,  888.],\n",
            "        [1440.,  885.],\n",
            "        [1443.,  885.],\n",
            "        [1449.,  879.],\n",
            "        [1449.,  873.],\n",
            "        [1452.,  870.],\n",
            "        [1452.,  858.],\n",
            "        [1470.,  840.],\n",
            "        [1473.,  840.],\n",
            "        [1479.,  846.],\n",
            "        [1479.,  849.],\n",
            "        [1476.,  852.],\n",
            "        [1476.,  870.],\n",
            "        [1479.,  873.],\n",
            "        [1479.,  876.],\n",
            "        [1482.,  879.],\n",
            "        [1485.,  879.],\n",
            "        [1488.,  882.],\n",
            "        [1494.,  882.],\n",
            "        [1494.,  879.],\n",
            "        [1497.,  876.],\n",
            "        [1497.,  858.],\n",
            "        [1500.,  855.],\n",
            "        [1500.,  843.],\n",
            "        [1503.,  840.],\n",
            "        [1503.,  837.],\n",
            "        [1506.,  834.],\n",
            "        [1506.,  828.],\n",
            "        [1509.,  825.],\n",
            "        [1509.,  810.],\n",
            "        [1512.,  807.],\n",
            "        [1512.,  801.],\n",
            "        [1518.,  795.],\n",
            "        [1527.,  795.],\n",
            "        [1527.,  732.],\n",
            "        [1518.,  732.],\n",
            "        [1512.,  726.],\n",
            "        [1512.,  702.]])\n",
            "tensor([[762., 450.],\n",
            "        [762., 465.],\n",
            "        [765., 468.],\n",
            "        [765., 474.],\n",
            "        [762., 477.],\n",
            "        [750., 477.],\n",
            "        [750., 546.],\n",
            "        [759., 546.],\n",
            "        [762., 549.],\n",
            "        [762., 552.],\n",
            "        [765., 555.],\n",
            "        [765., 603.],\n",
            "        [768., 606.],\n",
            "        [768., 615.],\n",
            "        [771., 618.],\n",
            "        [771., 627.],\n",
            "        [780., 627.],\n",
            "        [780., 618.],\n",
            "        [783., 615.],\n",
            "        [783., 606.],\n",
            "        [786., 603.],\n",
            "        [786., 597.],\n",
            "        [783., 594.],\n",
            "        [783., 591.],\n",
            "        [789., 585.],\n",
            "        [798., 585.],\n",
            "        [804., 579.],\n",
            "        [804., 576.],\n",
            "        [807., 573.],\n",
            "        [807., 564.],\n",
            "        [804., 561.],\n",
            "        [804., 543.],\n",
            "        [807., 540.],\n",
            "        [810., 540.],\n",
            "        [819., 549.],\n",
            "        [831., 549.],\n",
            "        [831., 528.],\n",
            "        [822., 528.],\n",
            "        [819., 525.],\n",
            "        [819., 522.],\n",
            "        [816., 519.],\n",
            "        [816., 510.],\n",
            "        [813., 507.],\n",
            "        [813., 498.],\n",
            "        [810., 495.],\n",
            "        [810., 492.],\n",
            "        [804., 486.],\n",
            "        [804., 483.],\n",
            "        [792., 471.],\n",
            "        [792., 468.],\n",
            "        [789., 465.],\n",
            "        [789., 450.]])\n",
            "tensor([[570., 210.],\n",
            "        [570., 273.],\n",
            "        [579., 273.],\n",
            "        [582., 276.],\n",
            "        [585., 276.],\n",
            "        [588., 279.],\n",
            "        [588., 291.],\n",
            "        [591., 294.],\n",
            "        [591., 303.],\n",
            "        [603., 303.],\n",
            "        [603., 294.],\n",
            "        [606., 291.],\n",
            "        [606., 267.],\n",
            "        [609., 264.],\n",
            "        [609., 261.],\n",
            "        [612., 258.],\n",
            "        [612., 246.],\n",
            "        [609., 243.],\n",
            "        [609., 237.],\n",
            "        [606., 234.],\n",
            "        [606., 231.],\n",
            "        [603., 228.],\n",
            "        [600., 228.],\n",
            "        [597., 225.],\n",
            "        [597., 210.]])\n",
            "tensor([[258., 438.],\n",
            "        [258., 531.],\n",
            "        [267., 531.],\n",
            "        [273., 537.],\n",
            "        [273., 543.],\n",
            "        [276., 546.],\n",
            "        [276., 549.],\n",
            "        [279., 552.],\n",
            "        [279., 555.],\n",
            "        [282., 558.],\n",
            "        [282., 564.],\n",
            "        [285., 567.],\n",
            "        [285., 576.],\n",
            "        [288., 579.],\n",
            "        [288., 597.],\n",
            "        [291., 600.],\n",
            "        [291., 615.],\n",
            "        [306., 615.],\n",
            "        [306., 603.],\n",
            "        [309., 600.],\n",
            "        [309., 579.],\n",
            "        [306., 576.],\n",
            "        [306., 567.],\n",
            "        [303., 564.],\n",
            "        [303., 561.],\n",
            "        [300., 558.],\n",
            "        [300., 549.],\n",
            "        [306., 543.],\n",
            "        [309., 546.],\n",
            "        [309., 555.],\n",
            "        [315., 561.],\n",
            "        [318., 561.],\n",
            "        [321., 558.],\n",
            "        [339., 558.],\n",
            "        [339., 540.],\n",
            "        [330., 540.],\n",
            "        [324., 534.],\n",
            "        [324., 498.],\n",
            "        [321., 495.],\n",
            "        [321., 483.],\n",
            "        [318., 480.],\n",
            "        [318., 474.],\n",
            "        [297., 453.],\n",
            "        [297., 438.]])\n",
            "tensor([[ 51., 798.],\n",
            "        [ 51., 813.],\n",
            "        [ 48., 816.],\n",
            "        [ 48., 819.],\n",
            "        [ 39., 828.],\n",
            "        [ 30., 828.],\n",
            "        [ 30., 882.],\n",
            "        [ 42., 882.],\n",
            "        [ 54., 894.],\n",
            "        [ 57., 894.],\n",
            "        [ 60., 897.],\n",
            "        [ 63., 897.],\n",
            "        [ 72., 906.],\n",
            "        [ 72., 909.],\n",
            "        [ 75., 909.],\n",
            "        [ 81., 915.],\n",
            "        [ 81., 921.],\n",
            "        [ 84., 924.],\n",
            "        [ 84., 927.],\n",
            "        [ 87., 930.],\n",
            "        [ 87., 933.],\n",
            "        [ 93., 939.],\n",
            "        [ 93., 942.],\n",
            "        [105., 954.],\n",
            "        [105., 957.],\n",
            "        [108., 960.],\n",
            "        [108., 963.],\n",
            "        [111., 966.],\n",
            "        [111., 969.],\n",
            "        [117., 975.],\n",
            "        [117., 978.],\n",
            "        [123., 984.],\n",
            "        [147., 984.],\n",
            "        [147., 972.],\n",
            "        [135., 972.],\n",
            "        [132., 969.],\n",
            "        [132., 966.],\n",
            "        [120., 954.],\n",
            "        [120., 930.],\n",
            "        [114., 924.],\n",
            "        [114., 921.],\n",
            "        [108., 915.],\n",
            "        [108., 873.],\n",
            "        [111., 870.],\n",
            "        [111., 867.],\n",
            "        [114., 864.],\n",
            "        [114., 861.],\n",
            "        [117., 858.],\n",
            "        [117., 843.],\n",
            "        [111., 837.],\n",
            "        [111., 834.],\n",
            "        [ 96., 819.],\n",
            "        [ 96., 816.],\n",
            "        [ 93., 813.],\n",
            "        [ 93., 810.],\n",
            "        [ 90., 807.],\n",
            "        [ 90., 798.]])\n",
            "tensor([[1098.,   42.],\n",
            "        [1098.,  141.],\n",
            "        [1113.,  141.],\n",
            "        [1116.,  138.],\n",
            "        [1119.,  138.],\n",
            "        [1119.,  135.],\n",
            "        [1122.,  132.],\n",
            "        [1122.,  129.],\n",
            "        [1125.,  126.],\n",
            "        [1125.,  111.],\n",
            "        [1128.,  108.],\n",
            "        [1128.,  102.],\n",
            "        [1134.,   96.],\n",
            "        [1143.,   96.],\n",
            "        [1143.,   78.],\n",
            "        [1134.,   78.],\n",
            "        [1128.,   72.],\n",
            "        [1128.,   69.],\n",
            "        [1125.,   66.],\n",
            "        [1125.,   60.],\n",
            "        [1119.,   54.],\n",
            "        [1119.,   42.]])\n",
            "tensor([[714., 234.],\n",
            "        [714., 327.],\n",
            "        [723., 327.],\n",
            "        [729., 333.],\n",
            "        [729., 348.],\n",
            "        [732., 351.],\n",
            "        [732., 363.],\n",
            "        [735., 366.],\n",
            "        [735., 369.],\n",
            "        [747., 381.],\n",
            "        [747., 384.],\n",
            "        [753., 384.],\n",
            "        [759., 378.],\n",
            "        [771., 378.],\n",
            "        [771., 324.],\n",
            "        [762., 324.],\n",
            "        [759., 321.],\n",
            "        [759., 312.],\n",
            "        [756., 309.],\n",
            "        [756., 294.],\n",
            "        [762., 288.],\n",
            "        [771., 288.],\n",
            "        [771., 270.],\n",
            "        [759., 270.],\n",
            "        [756., 267.],\n",
            "        [753., 267.],\n",
            "        [747., 261.],\n",
            "        [747., 258.],\n",
            "        [744., 255.],\n",
            "        [744., 249.],\n",
            "        [741., 246.],\n",
            "        [741., 234.]])\n",
            "tensor([[942., 174.],\n",
            "        [942., 186.],\n",
            "        [939., 189.],\n",
            "        [930., 189.],\n",
            "        [930., 291.],\n",
            "        [939., 291.],\n",
            "        [945., 297.],\n",
            "        [945., 300.],\n",
            "        [951., 306.],\n",
            "        [951., 315.],\n",
            "        [957., 315.],\n",
            "        [957., 306.],\n",
            "        [969., 294.],\n",
            "        [969., 273.],\n",
            "        [972., 270.],\n",
            "        [972., 252.],\n",
            "        [981., 243.],\n",
            "        [981., 237.],\n",
            "        [984., 234.],\n",
            "        [984., 231.],\n",
            "        [981., 228.],\n",
            "        [981., 213.],\n",
            "        [978., 210.],\n",
            "        [978., 204.],\n",
            "        [975., 201.],\n",
            "        [975., 198.],\n",
            "        [963., 186.],\n",
            "        [963., 174.]])\n",
            "tensor([[1497.,  474.],\n",
            "        [1497.,  486.],\n",
            "        [1491.,  492.],\n",
            "        [1488.,  492.],\n",
            "        [1485.,  495.],\n",
            "        [1482.,  495.],\n",
            "        [1479.,  498.],\n",
            "        [1479.,  501.],\n",
            "        [1473.,  507.],\n",
            "        [1473.,  510.],\n",
            "        [1470.,  513.],\n",
            "        [1458.,  513.],\n",
            "        [1458.,  576.],\n",
            "        [1470.,  576.],\n",
            "        [1473.,  579.],\n",
            "        [1473.,  582.],\n",
            "        [1476.,  585.],\n",
            "        [1476.,  597.],\n",
            "        [1479.,  600.],\n",
            "        [1479.,  606.],\n",
            "        [1482.,  609.],\n",
            "        [1482.,  621.],\n",
            "        [1485.,  624.],\n",
            "        [1485.,  636.],\n",
            "        [1488.,  639.],\n",
            "        [1488.,  651.],\n",
            "        [1497.,  651.],\n",
            "        [1497.,  639.],\n",
            "        [1500.,  636.],\n",
            "        [1500.,  576.],\n",
            "        [1503.,  573.],\n",
            "        [1503.,  570.],\n",
            "        [1506.,  567.],\n",
            "        [1506.,  561.],\n",
            "        [1509.,  558.],\n",
            "        [1509.,  555.],\n",
            "        [1518.,  546.],\n",
            "        [1518.,  543.],\n",
            "        [1521.,  540.],\n",
            "        [1521.,  537.],\n",
            "        [1527.,  531.],\n",
            "        [1533.,  531.],\n",
            "        [1536.,  528.],\n",
            "        [1551.,  528.],\n",
            "        [1551.,  510.],\n",
            "        [1542.,  510.],\n",
            "        [1533.,  501.],\n",
            "        [1533.,  495.],\n",
            "        [1527.,  489.],\n",
            "        [1527.,  486.],\n",
            "        [1524.,  483.],\n",
            "        [1524.,  474.]])\n",
            "tensor([[570., 288.],\n",
            "        [570., 354.],\n",
            "        [582., 354.],\n",
            "        [588., 360.],\n",
            "        [588., 363.],\n",
            "        [591., 366.],\n",
            "        [591., 372.],\n",
            "        [594., 375.],\n",
            "        [594., 381.],\n",
            "        [597., 384.],\n",
            "        [597., 390.],\n",
            "        [603., 396.],\n",
            "        [606., 396.],\n",
            "        [609., 399.],\n",
            "        [609., 402.],\n",
            "        [615., 408.],\n",
            "        [618., 408.],\n",
            "        [624., 402.],\n",
            "        [624., 378.],\n",
            "        [621., 375.],\n",
            "        [621., 327.],\n",
            "        [624., 324.],\n",
            "        [624., 303.],\n",
            "        [621., 300.],\n",
            "        [621., 297.],\n",
            "        [618., 294.],\n",
            "        [618., 291.],\n",
            "        [606., 291.],\n",
            "        [603., 294.],\n",
            "        [600., 294.],\n",
            "        [597., 297.],\n",
            "        [591., 297.],\n",
            "        [582., 288.]])\n",
            "tensor([[1152.,  510.],\n",
            "        [1152.,  528.],\n",
            "        [1149.,  531.],\n",
            "        [1149.,  537.],\n",
            "        [1143.,  543.],\n",
            "        [1134.,  543.],\n",
            "        [1134.,  594.],\n",
            "        [1143.,  594.],\n",
            "        [1146.,  597.],\n",
            "        [1149.,  597.],\n",
            "        [1152.,  600.],\n",
            "        [1155.,  600.],\n",
            "        [1158.,  603.],\n",
            "        [1158.,  606.],\n",
            "        [1161.,  609.],\n",
            "        [1161.,  624.],\n",
            "        [1164.,  627.],\n",
            "        [1164.,  663.],\n",
            "        [1179.,  663.],\n",
            "        [1179.,  654.],\n",
            "        [1182.,  651.],\n",
            "        [1182.,  630.],\n",
            "        [1185.,  627.],\n",
            "        [1185.,  606.],\n",
            "        [1188.,  603.],\n",
            "        [1188.,  594.],\n",
            "        [1191.,  591.],\n",
            "        [1188.,  588.],\n",
            "        [1188.,  567.],\n",
            "        [1191.,  564.],\n",
            "        [1191.,  561.],\n",
            "        [1194.,  558.],\n",
            "        [1194.,  543.],\n",
            "        [1188.,  537.],\n",
            "        [1188.,  534.],\n",
            "        [1185.,  531.],\n",
            "        [1185.,  525.],\n",
            "        [1182.,  522.],\n",
            "        [1182.,  510.]])\n",
            "tensor([[1206.,  126.],\n",
            "        [1206.,  261.],\n",
            "        [1224.,  261.],\n",
            "        [1227.,  264.],\n",
            "        [1233.,  258.],\n",
            "        [1233.,  252.],\n",
            "        [1236.,  249.],\n",
            "        [1236.,  231.],\n",
            "        [1239.,  228.],\n",
            "        [1239.,  222.],\n",
            "        [1242.,  219.],\n",
            "        [1242.,  216.],\n",
            "        [1245.,  213.],\n",
            "        [1245.,  183.],\n",
            "        [1254.,  174.],\n",
            "        [1263.,  174.],\n",
            "        [1263.,  168.],\n",
            "        [1254.,  168.],\n",
            "        [1248.,  162.],\n",
            "        [1248.,  159.],\n",
            "        [1239.,  150.],\n",
            "        [1239.,  147.],\n",
            "        [1236.,  144.],\n",
            "        [1236.,  126.]])\n",
            "tensor([[1815.,  150.],\n",
            "        [1815.,  162.],\n",
            "        [1806.,  171.],\n",
            "        [1794.,  171.],\n",
            "        [1794.,  261.],\n",
            "        [1803.,  261.],\n",
            "        [1806.,  264.],\n",
            "        [1806.,  267.],\n",
            "        [1809.,  270.],\n",
            "        [1809.,  279.],\n",
            "        [1821.,  279.],\n",
            "        [1821.,  258.],\n",
            "        [1824.,  255.],\n",
            "        [1824.,  246.],\n",
            "        [1827.,  243.],\n",
            "        [1827.,  240.],\n",
            "        [1830.,  237.],\n",
            "        [1830.,  234.],\n",
            "        [1833.,  231.],\n",
            "        [1833.,  225.],\n",
            "        [1836.,  222.],\n",
            "        [1836.,  216.],\n",
            "        [1842.,  210.],\n",
            "        [1842.,  201.],\n",
            "        [1845.,  198.],\n",
            "        [1845.,  171.],\n",
            "        [1848.,  168.],\n",
            "        [1848.,  150.]])\n",
            "tensor([[213.,  42.],\n",
            "        [213.,  54.],\n",
            "        [207.,  60.],\n",
            "        [198.,  60.],\n",
            "        [198., 108.],\n",
            "        [207., 108.],\n",
            "        [213., 114.],\n",
            "        [213., 117.],\n",
            "        [210., 120.],\n",
            "        [210., 126.],\n",
            "        [207., 129.],\n",
            "        [198., 129.],\n",
            "        [198., 171.],\n",
            "        [210., 171.],\n",
            "        [210., 159.],\n",
            "        [213., 156.],\n",
            "        [213., 153.],\n",
            "        [216., 150.],\n",
            "        [216., 147.],\n",
            "        [222., 141.],\n",
            "        [222., 135.],\n",
            "        [225., 132.],\n",
            "        [225., 129.],\n",
            "        [228., 126.],\n",
            "        [228., 123.],\n",
            "        [231., 120.],\n",
            "        [234., 120.],\n",
            "        [237., 123.],\n",
            "        [237., 126.],\n",
            "        [240., 129.],\n",
            "        [240., 135.],\n",
            "        [243., 138.],\n",
            "        [243., 141.],\n",
            "        [246., 141.],\n",
            "        [249., 144.],\n",
            "        [252., 144.],\n",
            "        [255., 147.],\n",
            "        [258., 144.],\n",
            "        [258., 141.],\n",
            "        [261., 138.],\n",
            "        [255., 132.],\n",
            "        [255., 129.],\n",
            "        [252., 126.],\n",
            "        [252., 123.],\n",
            "        [249., 120.],\n",
            "        [249., 114.],\n",
            "        [246., 111.],\n",
            "        [246., 108.],\n",
            "        [243., 105.],\n",
            "        [243., 102.],\n",
            "        [240.,  99.],\n",
            "        [240.,  93.],\n",
            "        [237.,  90.],\n",
            "        [237.,  75.],\n",
            "        [234.,  72.],\n",
            "        [234.,  66.],\n",
            "        [231.,  63.],\n",
            "        [231.,  42.]])\n",
            "tensor([[ 90., 138.],\n",
            "        [ 90., 192.],\n",
            "        [ 99., 192.],\n",
            "        [102., 195.],\n",
            "        [102., 198.],\n",
            "        [105., 201.],\n",
            "        [105., 207.],\n",
            "        [108., 210.],\n",
            "        [108., 213.],\n",
            "        [111., 216.],\n",
            "        [111., 219.],\n",
            "        [117., 225.],\n",
            "        [117., 237.],\n",
            "        [120., 240.],\n",
            "        [120., 255.],\n",
            "        [141., 255.],\n",
            "        [141., 237.],\n",
            "        [144., 234.],\n",
            "        [144., 231.],\n",
            "        [141., 228.],\n",
            "        [141., 204.],\n",
            "        [138., 201.],\n",
            "        [138., 195.],\n",
            "        [135., 192.],\n",
            "        [135., 189.],\n",
            "        [132., 186.],\n",
            "        [132., 183.],\n",
            "        [135., 180.],\n",
            "        [135., 177.],\n",
            "        [138., 174.],\n",
            "        [135., 171.],\n",
            "        [135., 168.],\n",
            "        [129., 162.],\n",
            "        [129., 159.],\n",
            "        [126., 156.],\n",
            "        [126., 153.],\n",
            "        [123., 150.],\n",
            "        [123., 138.]])\n",
            "tensor([[1170.,  150.],\n",
            "        [1170.,  279.],\n",
            "        [1179.,  279.],\n",
            "        [1182.,  276.],\n",
            "        [1188.,  276.],\n",
            "        [1191.,  273.],\n",
            "        [1194.,  273.],\n",
            "        [1200.,  267.],\n",
            "        [1200.,  264.],\n",
            "        [1203.,  261.],\n",
            "        [1203.,  258.],\n",
            "        [1206.,  255.],\n",
            "        [1215.,  255.],\n",
            "        [1206.,  255.],\n",
            "        [1203.,  252.],\n",
            "        [1203.,  240.],\n",
            "        [1200.,  237.],\n",
            "        [1200.,  222.],\n",
            "        [1203.,  219.],\n",
            "        [1203.,  216.],\n",
            "        [1206.,  213.],\n",
            "        [1215.,  213.],\n",
            "        [1215.,  183.],\n",
            "        [1203.,  183.],\n",
            "        [1200.,  180.],\n",
            "        [1200.,  174.],\n",
            "        [1197.,  171.],\n",
            "        [1197.,  168.],\n",
            "        [1194.,  165.],\n",
            "        [1194.,  162.],\n",
            "        [1191.,  159.],\n",
            "        [1191.,  150.]])\n",
            "tensor([[726.,   6.],\n",
            "        [726.,  72.],\n",
            "        [735.,  72.],\n",
            "        [741.,  78.],\n",
            "        [741.,  90.],\n",
            "        [744.,  93.],\n",
            "        [744., 102.],\n",
            "        [747., 105.],\n",
            "        [747., 108.],\n",
            "        [750., 108.],\n",
            "        [753., 105.],\n",
            "        [753., 102.],\n",
            "        [756.,  99.],\n",
            "        [756.,  75.],\n",
            "        [765.,  66.],\n",
            "        [765.,  45.],\n",
            "        [762.,  42.],\n",
            "        [762.,  33.],\n",
            "        [759.,  30.],\n",
            "        [759.,  27.],\n",
            "        [756.,  24.],\n",
            "        [756.,  21.],\n",
            "        [753.,  18.],\n",
            "        [753.,   6.]])\n",
            "tensor([[396., 510.],\n",
            "        [396., 525.],\n",
            "        [393., 528.],\n",
            "        [393., 537.],\n",
            "        [390., 540.],\n",
            "        [378., 540.],\n",
            "        [378., 591.],\n",
            "        [387., 591.],\n",
            "        [390., 594.],\n",
            "        [390., 597.],\n",
            "        [393., 600.],\n",
            "        [393., 609.],\n",
            "        [396., 612.],\n",
            "        [396., 627.],\n",
            "        [399., 630.],\n",
            "        [399., 642.],\n",
            "        [396., 645.],\n",
            "        [396., 660.],\n",
            "        [399., 663.],\n",
            "        [399., 666.],\n",
            "        [402., 669.],\n",
            "        [414., 669.],\n",
            "        [417., 666.],\n",
            "        [417., 654.],\n",
            "        [420., 651.],\n",
            "        [420., 648.],\n",
            "        [423., 645.],\n",
            "        [429., 651.],\n",
            "        [429., 663.],\n",
            "        [435., 669.],\n",
            "        [438., 669.],\n",
            "        [450., 657.],\n",
            "        [459., 657.],\n",
            "        [459., 645.],\n",
            "        [450., 645.],\n",
            "        [444., 639.],\n",
            "        [444., 636.],\n",
            "        [441., 633.],\n",
            "        [441., 618.],\n",
            "        [438., 615.],\n",
            "        [438., 606.],\n",
            "        [435., 603.],\n",
            "        [435., 597.],\n",
            "        [432., 594.],\n",
            "        [432., 585.],\n",
            "        [429., 582.],\n",
            "        [429., 576.],\n",
            "        [432., 573.],\n",
            "        [432., 564.],\n",
            "        [429., 561.],\n",
            "        [429., 555.],\n",
            "        [426., 552.],\n",
            "        [426., 546.],\n",
            "        [420., 540.],\n",
            "        [420., 537.],\n",
            "        [417., 534.],\n",
            "        [417., 510.]])\n",
            "tensor([[342., 498.],\n",
            "        [342., 570.],\n",
            "        [351., 570.],\n",
            "        [357., 576.],\n",
            "        [357., 579.],\n",
            "        [360., 582.],\n",
            "        [360., 591.],\n",
            "        [363., 594.],\n",
            "        [363., 600.],\n",
            "        [366., 603.],\n",
            "        [366., 606.],\n",
            "        [369., 609.],\n",
            "        [369., 618.],\n",
            "        [372., 621.],\n",
            "        [372., 636.],\n",
            "        [375., 639.],\n",
            "        [375., 651.],\n",
            "        [393., 651.],\n",
            "        [393., 621.],\n",
            "        [390., 618.],\n",
            "        [390., 594.],\n",
            "        [387., 591.],\n",
            "        [387., 576.],\n",
            "        [384., 573.],\n",
            "        [384., 546.],\n",
            "        [387., 543.],\n",
            "        [387., 540.],\n",
            "        [390., 537.],\n",
            "        [390., 531.],\n",
            "        [387., 531.],\n",
            "        [378., 522.],\n",
            "        [378., 519.],\n",
            "        [375., 516.],\n",
            "        [375., 498.]])\n"
          ]
        }
      ],
      "source": [
        "from collections import defaultdict\n",
        "\n",
        "import cv2\n",
        "\n",
        "from ultralytics import YOLO\n",
        "from ultralytics.utils.plotting import Annotator, colors\n",
        "\n",
        "track_history = defaultdict(lambda: [])\n",
        "\n",
        "model = YOLO(\"yolov8l-seg.pt\")  # segmentation model\n",
        "cap = cv2.VideoCapture(\"path/to/video/file.mp4\")\n",
        "w, h, fps = (int(cap.get(x)) for x in (cv2.CAP_PROP_FRAME_WIDTH, cv2.CAP_PROP_FRAME_HEIGHT, cv2.CAP_PROP_FPS))\n",
        "\n",
        "out = cv2.VideoWriter(\"instance-segmentation-object-tracking.avi\", cv2.VideoWriter_fourcc(*\"MJPG\"), fps, (w, h))\n",
        "\n",
        "while True:\n",
        "    ret, im0 = cap.read()\n",
        "    if not ret:\n",
        "        print(\"Video frame is empty or video processing has been successfully completed.\")\n",
        "        break\n",
        "\n",
        "    annotator = Annotator(im0, line_width=2)\n",
        "\n",
        "    results = model.track(im0, persist=True)\n",
        "\n",
        "    if results[0].boxes.id is not None and results[0].masks is not None:\n",
        "        masks = results[0].masks.xy\n",
        "        track_ids = results[0].boxes.id.int().cpu().tolist()\n",
        "\n",
        "        for mask, track_id in zip(masks, track_ids):\n",
        "            color = colors(int(track_id), True)\n",
        "            txt_color = annotator.get_txt_color(color)\n",
        "            annotator.seg_bbox(mask=mask, mask_color=color, label=str(track_id), txt_color=txt_color)\n",
        "\n",
        "    out.write(im0)\n",
        "    cv2.imshow(\"instance-segmentation-object-tracking\", im0)\n",
        "\n",
        "    if cv2.waitKey(1) & 0xFF == ord(\"q\"):\n",
        "        break\n",
        "\n",
        "out.release()\n",
        "cap.release()\n",
        "cv2.destroyAllWindows()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Instance Segmentation"
      ],
      "metadata": {
        "id": "7RzVmCYWFVN_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "\n",
        "from ultralytics import YOLO\n",
        "from ultralytics.utils.plotting import Annotator, colors\n",
        "\n",
        "model = YOLO(\"yolov8n-seg.pt\")  # segmentation model\n",
        "names = model.model.names\n",
        "cap = cv2.VideoCapture(\"/content/drive/MyDrive/Projects/instance seg+obj track+yolov8/videos/853889-hd_1920_1080_25fps.mp4\")\n",
        "w, h, fps = (int(cap.get(x)) for x in (cv2.CAP_PROP_FRAME_WIDTH, cv2.CAP_PROP_FRAME_HEIGHT, cv2.CAP_PROP_FPS))\n",
        "\n",
        "out = cv2.VideoWriter(\"/content/drive/MyDrive/Projects/instance seg+obj track+yolov8/output/instance-segmentation.avi\", cv2.VideoWriter_fourcc(*\"MJPG\"), fps, (w, h))\n",
        "\n",
        "while True:\n",
        "    ret, im0 = cap.read()\n",
        "    if not ret:\n",
        "        print(\"Video frame is empty or video processing has been successfully completed.\")\n",
        "        break\n",
        "\n",
        "    results = model.predict(im0)\n",
        "    annotator = Annotator(im0, line_width=2)\n",
        "\n",
        "    if results[0].masks is not None:\n",
        "        clss = results[0].boxes.cls.cpu().tolist()\n",
        "        masks = results[0].masks.xy\n",
        "        for mask, cls in zip(masks, clss):\n",
        "            color = colors(int(cls), True)\n",
        "            txt_color = annotator.get_txt_color(color)\n",
        "            annotator.seg_bbox(mask=mask, mask_color=color, label=names[int(cls)], txt_color=txt_color)\n",
        "\n",
        "    out.write(im0)\n",
        "\n",
        "\n",
        "    if cv2.waitKey(1) & 0xFF == ord(\"q\"):\n",
        "        break\n",
        "\n",
        "out.release()\n",
        "cap.release()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zKYuJRsRCWhi",
        "outputId": "4d605423-4c8f-401f-bffc-3cfff902dd2b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "0: 384x640 38 persons, 8.9ms\n",
            "Speed: 1.9ms preprocess, 8.9ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 38 persons, 9.8ms\n",
            "Speed: 3.1ms preprocess, 9.8ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 38 persons, 13.4ms\n",
            "Speed: 2.8ms preprocess, 13.4ms inference, 2.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 39 persons, 11.6ms\n",
            "Speed: 3.1ms preprocess, 11.6ms inference, 2.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 39 persons, 1 dog, 8.3ms\n",
            "Speed: 3.2ms preprocess, 8.3ms inference, 2.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 38 persons, 1 dog, 8.7ms\n",
            "Speed: 3.1ms preprocess, 8.7ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 40 persons, 16.0ms\n",
            "Speed: 3.6ms preprocess, 16.0ms inference, 2.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 39 persons, 1 dog, 10.1ms\n",
            "Speed: 3.3ms preprocess, 10.1ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 38 persons, 1 dog, 10.9ms\n",
            "Speed: 3.2ms preprocess, 10.9ms inference, 2.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 39 persons, 8.9ms\n",
            "Speed: 3.1ms preprocess, 8.9ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 37 persons, 10.8ms\n",
            "Speed: 3.2ms preprocess, 10.8ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 37 persons, 10.1ms\n",
            "Speed: 4.2ms preprocess, 10.1ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 38 persons, 9.1ms\n",
            "Speed: 3.2ms preprocess, 9.1ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 39 persons, 1 dog, 9.1ms\n",
            "Speed: 3.1ms preprocess, 9.1ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 38 persons, 1 dog, 10.9ms\n",
            "Speed: 3.2ms preprocess, 10.9ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 38 persons, 1 dog, 10.7ms\n",
            "Speed: 3.2ms preprocess, 10.7ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 40 persons, 1 dog, 11.6ms\n",
            "Speed: 3.2ms preprocess, 11.6ms inference, 2.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 38 persons, 1 dog, 12.2ms\n",
            "Speed: 4.1ms preprocess, 12.2ms inference, 2.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 39 persons, 1 dog, 14.9ms\n",
            "Speed: 3.2ms preprocess, 14.9ms inference, 3.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 37 persons, 1 dog, 12.7ms\n",
            "Speed: 4.5ms preprocess, 12.7ms inference, 2.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 38 persons, 1 dog, 14.8ms\n",
            "Speed: 2.9ms preprocess, 14.8ms inference, 2.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 37 persons, 1 dog, 12.6ms\n",
            "Speed: 3.1ms preprocess, 12.6ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 35 persons, 12.1ms\n",
            "Speed: 3.5ms preprocess, 12.1ms inference, 3.1ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 39 persons, 10.3ms\n",
            "Speed: 3.3ms preprocess, 10.3ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 39 persons, 8.6ms\n",
            "Speed: 3.1ms preprocess, 8.6ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 39 persons, 9.4ms\n",
            "Speed: 3.1ms preprocess, 9.4ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 37 persons, 10.7ms\n",
            "Speed: 3.8ms preprocess, 10.7ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 36 persons, 9.6ms\n",
            "Speed: 3.3ms preprocess, 9.6ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 36 persons, 9.4ms\n",
            "Speed: 3.2ms preprocess, 9.4ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 37 persons, 9.3ms\n",
            "Speed: 3.2ms preprocess, 9.3ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 37 persons, 11.6ms\n",
            "Speed: 4.1ms preprocess, 11.6ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 38 persons, 14.9ms\n",
            "Speed: 3.3ms preprocess, 14.9ms inference, 2.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 38 persons, 11.4ms\n",
            "Speed: 3.3ms preprocess, 11.4ms inference, 2.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 38 persons, 10.8ms\n",
            "Speed: 3.1ms preprocess, 10.8ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 39 persons, 14.9ms\n",
            "Speed: 3.5ms preprocess, 14.9ms inference, 3.1ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 40 persons, 10.6ms\n",
            "Speed: 3.1ms preprocess, 10.6ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 37 persons, 1 dog, 10.1ms\n",
            "Speed: 3.1ms preprocess, 10.1ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 36 persons, 10.8ms\n",
            "Speed: 2.9ms preprocess, 10.8ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 31 persons, 11.0ms\n",
            "Speed: 3.9ms preprocess, 11.0ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 34 persons, 10.2ms\n",
            "Speed: 3.1ms preprocess, 10.2ms inference, 2.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 36 persons, 10.0ms\n",
            "Speed: 3.3ms preprocess, 10.0ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 41 persons, 8.7ms\n",
            "Speed: 3.2ms preprocess, 8.7ms inference, 2.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 41 persons, 1 backpack, 10.9ms\n",
            "Speed: 3.6ms preprocess, 10.9ms inference, 2.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 35 persons, 1 backpack, 11.0ms\n",
            "Speed: 2.9ms preprocess, 11.0ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 37 persons, 1 backpack, 9.1ms\n",
            "Speed: 5.2ms preprocess, 9.1ms inference, 2.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 39 persons, 1 backpack, 13.6ms\n",
            "Speed: 3.0ms preprocess, 13.6ms inference, 3.1ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 39 persons, 12.9ms\n",
            "Speed: 3.6ms preprocess, 12.9ms inference, 2.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 38 persons, 1 backpack, 10.3ms\n",
            "Speed: 3.1ms preprocess, 10.3ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 38 persons, 1 backpack, 10.1ms\n",
            "Speed: 3.0ms preprocess, 10.1ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 40 persons, 1 dog, 1 backpack, 11.6ms\n",
            "Speed: 3.0ms preprocess, 11.6ms inference, 2.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 40 persons, 1 dog, 1 backpack, 10.5ms\n",
            "Speed: 3.3ms preprocess, 10.5ms inference, 3.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 38 persons, 9.4ms\n",
            "Speed: 3.1ms preprocess, 9.4ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 39 persons, 1 backpack, 8.3ms\n",
            "Speed: 3.2ms preprocess, 8.3ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 41 persons, 1 backpack, 10.5ms\n",
            "Speed: 3.2ms preprocess, 10.5ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 42 persons, 1 backpack, 10.6ms\n",
            "Speed: 3.2ms preprocess, 10.6ms inference, 2.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 39 persons, 8.6ms\n",
            "Speed: 3.4ms preprocess, 8.6ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 38 persons, 1 suitcase, 8.5ms\n",
            "Speed: 3.5ms preprocess, 8.5ms inference, 2.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 36 persons, 12.8ms\n",
            "Speed: 4.6ms preprocess, 12.8ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 33 persons, 14.3ms\n",
            "Speed: 4.1ms preprocess, 14.3ms inference, 3.1ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 37 persons, 11.5ms\n",
            "Speed: 3.4ms preprocess, 11.5ms inference, 2.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 35 persons, 9.7ms\n",
            "Speed: 4.8ms preprocess, 9.7ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 38 persons, 11.3ms\n",
            "Speed: 3.1ms preprocess, 11.3ms inference, 2.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 39 persons, 10.0ms\n",
            "Speed: 3.3ms preprocess, 10.0ms inference, 3.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 35 persons, 10.8ms\n",
            "Speed: 3.6ms preprocess, 10.8ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 37 persons, 1 backpack, 8.9ms\n",
            "Speed: 3.2ms preprocess, 8.9ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 36 persons, 10.4ms\n",
            "Speed: 3.3ms preprocess, 10.4ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 38 persons, 10.8ms\n",
            "Speed: 3.3ms preprocess, 10.8ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 37 persons, 1 dog, 9.1ms\n",
            "Speed: 3.2ms preprocess, 9.1ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 38 persons, 1 backpack, 9.3ms\n",
            "Speed: 3.3ms preprocess, 9.3ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 35 persons, 1 dog, 1 backpack, 10.8ms\n",
            "Speed: 3.3ms preprocess, 10.8ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 37 persons, 1 backpack, 10.2ms\n",
            "Speed: 3.4ms preprocess, 10.2ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 38 persons, 1 backpack, 10.0ms\n",
            "Speed: 3.1ms preprocess, 10.0ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 37 persons, 1 backpack, 15.4ms\n",
            "Speed: 4.2ms preprocess, 15.4ms inference, 2.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 40 persons, 12.0ms\n",
            "Speed: 4.2ms preprocess, 12.0ms inference, 2.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 42 persons, 1 dog, 9.8ms\n",
            "Speed: 3.1ms preprocess, 9.8ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 40 persons, 9.1ms\n",
            "Speed: 3.3ms preprocess, 9.1ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 40 persons, 1 dog, 9.2ms\n",
            "Speed: 3.3ms preprocess, 9.2ms inference, 2.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 39 persons, 1 dog, 11.2ms\n",
            "Speed: 5.4ms preprocess, 11.2ms inference, 2.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 40 persons, 1 dog, 11.6ms\n",
            "Speed: 3.8ms preprocess, 11.6ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 38 persons, 11.4ms\n",
            "Speed: 3.5ms preprocess, 11.4ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 37 persons, 1 dog, 9.5ms\n",
            "Speed: 3.2ms preprocess, 9.5ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 38 persons, 12.9ms\n",
            "Speed: 3.2ms preprocess, 12.9ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 38 persons, 9.7ms\n",
            "Speed: 3.2ms preprocess, 9.7ms inference, 3.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 37 persons, 8.4ms\n",
            "Speed: 3.2ms preprocess, 8.4ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 36 persons, 1 dog, 9.2ms\n",
            "Speed: 4.1ms preprocess, 9.2ms inference, 2.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 38 persons, 1 dog, 13.9ms\n",
            "Speed: 3.7ms preprocess, 13.9ms inference, 3.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 39 persons, 1 dog, 1 backpack, 14.7ms\n",
            "Speed: 4.7ms preprocess, 14.7ms inference, 2.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 41 persons, 1 backpack, 9.1ms\n",
            "Speed: 3.6ms preprocess, 9.1ms inference, 2.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 40 persons, 8.5ms\n",
            "Speed: 3.1ms preprocess, 8.5ms inference, 2.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 38 persons, 10.5ms\n",
            "Speed: 3.2ms preprocess, 10.5ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 38 persons, 1 backpack, 12.9ms\n",
            "Speed: 6.2ms preprocess, 12.9ms inference, 3.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 37 persons, 12.5ms\n",
            "Speed: 2.9ms preprocess, 12.5ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 39 persons, 1 backpack, 8.0ms\n",
            "Speed: 5.1ms preprocess, 8.0ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 38 persons, 2 backpacks, 11.7ms\n",
            "Speed: 3.0ms preprocess, 11.7ms inference, 2.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 37 persons, 1 backpack, 9.4ms\n",
            "Speed: 3.2ms preprocess, 9.4ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 39 persons, 1 backpack, 8.4ms\n",
            "Speed: 3.3ms preprocess, 8.4ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 39 persons, 1 backpack, 8.8ms\n",
            "Speed: 3.3ms preprocess, 8.8ms inference, 2.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 37 persons, 2 backpacks, 11.6ms\n",
            "Speed: 3.6ms preprocess, 11.6ms inference, 2.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 36 persons, 2 dogs, 2 backpacks, 15.4ms\n",
            "Speed: 3.9ms preprocess, 15.4ms inference, 2.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 35 persons, 2 dogs, 2 backpacks, 13.2ms\n",
            "Speed: 3.7ms preprocess, 13.2ms inference, 2.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 35 persons, 2 backpacks, 13.5ms\n",
            "Speed: 3.4ms preprocess, 13.5ms inference, 2.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 34 persons, 2 backpacks, 11.2ms\n",
            "Speed: 6.5ms preprocess, 11.2ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 35 persons, 2 backpacks, 11.1ms\n",
            "Speed: 4.3ms preprocess, 11.1ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 36 persons, 2 backpacks, 9.5ms\n",
            "Speed: 5.8ms preprocess, 9.5ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 35 persons, 2 backpacks, 8.6ms\n",
            "Speed: 6.2ms preprocess, 8.6ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 34 persons, 1 backpack, 10.4ms\n",
            "Speed: 4.0ms preprocess, 10.4ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 34 persons, 1 backpack, 10.5ms\n",
            "Speed: 3.4ms preprocess, 10.5ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 33 persons, 1 backpack, 9.1ms\n",
            "Speed: 3.2ms preprocess, 9.1ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 33 persons, 2 dogs, 1 backpack, 10.0ms\n",
            "Speed: 3.2ms preprocess, 10.0ms inference, 2.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 32 persons, 2 dogs, 1 backpack, 11.2ms\n",
            "Speed: 4.4ms preprocess, 11.2ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 35 persons, 2 backpacks, 10.9ms\n",
            "Speed: 3.1ms preprocess, 10.9ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 37 persons, 1 dog, 1 backpack, 1 handbag, 10.9ms\n",
            "Speed: 3.1ms preprocess, 10.9ms inference, 2.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 36 persons, 1 dog, 14.7ms\n",
            "Speed: 3.1ms preprocess, 14.7ms inference, 2.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 36 persons, 14.1ms\n",
            "Speed: 2.9ms preprocess, 14.1ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 36 persons, 1 dog, 1 backpack, 9.8ms\n",
            "Speed: 3.2ms preprocess, 9.8ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 34 persons, 1 dog, 1 backpack, 8.5ms\n",
            "Speed: 3.1ms preprocess, 8.5ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 35 persons, 8.9ms\n",
            "Speed: 3.3ms preprocess, 8.9ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 37 persons, 11.4ms\n",
            "Speed: 3.1ms preprocess, 11.4ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 33 persons, 1 dog, 21.8ms\n",
            "Speed: 10.3ms preprocess, 21.8ms inference, 2.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 33 persons, 1 dog, 11.1ms\n",
            "Speed: 2.9ms preprocess, 11.1ms inference, 2.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 34 persons, 1 dog, 20.5ms\n",
            "Speed: 3.3ms preprocess, 20.5ms inference, 2.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 34 persons, 1 dog, 19.4ms\n",
            "Speed: 4.1ms preprocess, 19.4ms inference, 2.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 34 persons, 1 dog, 14.0ms\n",
            "Speed: 5.8ms preprocess, 14.0ms inference, 2.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 35 persons, 1 dog, 20.9ms\n",
            "Speed: 3.4ms preprocess, 20.9ms inference, 2.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 35 persons, 14.9ms\n",
            "Speed: 6.3ms preprocess, 14.9ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 32 persons, 1 dog, 11.9ms\n",
            "Speed: 4.1ms preprocess, 11.9ms inference, 2.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 35 persons, 1 handbag, 11.6ms\n",
            "Speed: 4.8ms preprocess, 11.6ms inference, 2.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 32 persons, 1 skateboard, 9.8ms\n",
            "Speed: 4.3ms preprocess, 9.8ms inference, 2.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 32 persons, 1 dog, 13.5ms\n",
            "Speed: 5.6ms preprocess, 13.5ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 31 persons, 1 cat, 13.4ms\n",
            "Speed: 3.3ms preprocess, 13.4ms inference, 2.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 30 persons, 1 cat, 1 dog, 16.8ms\n",
            "Speed: 3.8ms preprocess, 16.8ms inference, 2.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 30 persons, 1 cat, 1 dog, 10.3ms\n",
            "Speed: 3.8ms preprocess, 10.3ms inference, 2.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 33 persons, 10.7ms\n",
            "Speed: 3.4ms preprocess, 10.7ms inference, 3.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 32 persons, 1 backpack, 19.3ms\n",
            "Speed: 4.1ms preprocess, 19.3ms inference, 2.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 31 persons, 11.2ms\n",
            "Speed: 2.9ms preprocess, 11.2ms inference, 2.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 33 persons, 10.0ms\n",
            "Speed: 3.0ms preprocess, 10.0ms inference, 2.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 31 persons, 10.2ms\n",
            "Speed: 3.1ms preprocess, 10.2ms inference, 2.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 30 persons, 14.8ms\n",
            "Speed: 3.1ms preprocess, 14.8ms inference, 2.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 31 persons, 11.0ms\n",
            "Speed: 4.4ms preprocess, 11.0ms inference, 2.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 31 persons, 14.5ms\n",
            "Speed: 2.9ms preprocess, 14.5ms inference, 2.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 33 persons, 1 handbag, 9.7ms\n",
            "Speed: 3.0ms preprocess, 9.7ms inference, 2.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 31 persons, 1 handbag, 12.4ms\n",
            "Speed: 3.2ms preprocess, 12.4ms inference, 2.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 33 persons, 1 handbag, 11.7ms\n",
            "Speed: 2.9ms preprocess, 11.7ms inference, 2.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 31 persons, 1 handbag, 14.2ms\n",
            "Speed: 5.4ms preprocess, 14.2ms inference, 4.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 31 persons, 1 handbag, 10.9ms\n",
            "Speed: 3.2ms preprocess, 10.9ms inference, 2.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 31 persons, 1 handbag, 17.4ms\n",
            "Speed: 3.1ms preprocess, 17.4ms inference, 3.1ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 31 persons, 14.4ms\n",
            "Speed: 3.1ms preprocess, 14.4ms inference, 2.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 30 persons, 10.1ms\n",
            "Speed: 3.3ms preprocess, 10.1ms inference, 2.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 30 persons, 19.8ms\n",
            "Speed: 3.5ms preprocess, 19.8ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 30 persons, 15.8ms\n",
            "Speed: 3.3ms preprocess, 15.8ms inference, 2.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 30 persons, 13.2ms\n",
            "Speed: 3.2ms preprocess, 13.2ms inference, 2.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 31 persons, 12.9ms\n",
            "Speed: 7.9ms preprocess, 12.9ms inference, 2.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 30 persons, 24.5ms\n",
            "Speed: 3.3ms preprocess, 24.5ms inference, 2.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 28 persons, 1 dog, 20.6ms\n",
            "Speed: 7.3ms preprocess, 20.6ms inference, 4.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 30 persons, 14.0ms\n",
            "Speed: 3.0ms preprocess, 14.0ms inference, 2.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 30 persons, 22.1ms\n",
            "Speed: 3.3ms preprocess, 22.1ms inference, 2.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 30 persons, 14.6ms\n",
            "Speed: 3.2ms preprocess, 14.6ms inference, 3.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 28 persons, 16.4ms\n",
            "Speed: 3.0ms preprocess, 16.4ms inference, 3.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 27 persons, 1 dog, 16.4ms\n",
            "Speed: 5.8ms preprocess, 16.4ms inference, 2.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 29 persons, 16.6ms\n",
            "Speed: 7.0ms preprocess, 16.6ms inference, 3.1ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 29 persons, 10.2ms\n",
            "Speed: 3.2ms preprocess, 10.2ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 32 persons, 10.3ms\n",
            "Speed: 3.0ms preprocess, 10.3ms inference, 2.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 32 persons, 8.4ms\n",
            "Speed: 3.2ms preprocess, 8.4ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 30 persons, 8.4ms\n",
            "Speed: 3.8ms preprocess, 8.4ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 31 persons, 16.1ms\n",
            "Speed: 3.1ms preprocess, 16.1ms inference, 2.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 32 persons, 1 handbag, 13.3ms\n",
            "Speed: 3.1ms preprocess, 13.3ms inference, 2.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 31 persons, 1 handbag, 11.1ms\n",
            "Speed: 3.1ms preprocess, 11.1ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 34 persons, 1 backpack, 1 handbag, 8.3ms\n",
            "Speed: 3.1ms preprocess, 8.3ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 34 persons, 1 handbag, 10.8ms\n",
            "Speed: 3.1ms preprocess, 10.8ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 35 persons, 1 backpack, 1 handbag, 9.2ms\n",
            "Speed: 3.1ms preprocess, 9.2ms inference, 2.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 31 persons, 1 handbag, 9.1ms\n",
            "Speed: 3.1ms preprocess, 9.1ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 31 persons, 1 handbag, 10.3ms\n",
            "Speed: 3.1ms preprocess, 10.3ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 33 persons, 1 handbag, 11.8ms\n",
            "Speed: 3.2ms preprocess, 11.8ms inference, 2.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 32 persons, 9.5ms\n",
            "Speed: 3.3ms preprocess, 9.5ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 30 persons, 13.5ms\n",
            "Speed: 3.0ms preprocess, 13.5ms inference, 2.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 31 persons, 8.6ms\n",
            "Speed: 3.1ms preprocess, 8.6ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 33 persons, 11.7ms\n",
            "Speed: 3.5ms preprocess, 11.7ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 29 persons, 8.5ms\n",
            "Speed: 3.1ms preprocess, 8.5ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 29 persons, 1 suitcase, 13.3ms\n",
            "Speed: 3.1ms preprocess, 13.3ms inference, 2.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 29 persons, 11.2ms\n",
            "Speed: 3.6ms preprocess, 11.2ms inference, 2.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 31 persons, 1 dog, 12.6ms\n",
            "Speed: 3.4ms preprocess, 12.6ms inference, 2.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 31 persons, 1 dog, 8.8ms\n",
            "Speed: 3.1ms preprocess, 8.8ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 30 persons, 1 dog, 9.0ms\n",
            "Speed: 2.8ms preprocess, 9.0ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 30 persons, 1 dog, 8.7ms\n",
            "Speed: 2.7ms preprocess, 8.7ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 31 persons, 11.3ms\n",
            "Speed: 3.2ms preprocess, 11.3ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 28 persons, 11.5ms\n",
            "Speed: 2.9ms preprocess, 11.5ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 29 persons, 1 dog, 11.1ms\n",
            "Speed: 2.8ms preprocess, 11.1ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 33 persons, 8.1ms\n",
            "Speed: 3.1ms preprocess, 8.1ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 33 persons, 10.0ms\n",
            "Speed: 3.2ms preprocess, 10.0ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 34 persons, 9.2ms\n",
            "Speed: 3.0ms preprocess, 9.2ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 36 persons, 1 backpack, 10.4ms\n",
            "Speed: 3.1ms preprocess, 10.4ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 36 persons, 10.9ms\n",
            "Speed: 2.9ms preprocess, 10.9ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 35 persons, 10.7ms\n",
            "Speed: 3.2ms preprocess, 10.7ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 34 persons, 12.1ms\n",
            "Speed: 4.9ms preprocess, 12.1ms inference, 2.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 36 persons, 1 backpack, 13.3ms\n",
            "Speed: 3.1ms preprocess, 13.3ms inference, 2.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 34 persons, 1 backpack, 9.0ms\n",
            "Speed: 3.1ms preprocess, 9.0ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 34 persons, 1 backpack, 11.1ms\n",
            "Speed: 3.6ms preprocess, 11.1ms inference, 2.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 36 persons, 10.1ms\n",
            "Speed: 3.2ms preprocess, 10.1ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 36 persons, 8.6ms\n",
            "Speed: 3.2ms preprocess, 8.6ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 34 persons, 1 backpack, 10.3ms\n",
            "Speed: 2.9ms preprocess, 10.3ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 34 persons, 1 backpack, 11.5ms\n",
            "Speed: 3.4ms preprocess, 11.5ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 33 persons, 11.1ms\n",
            "Speed: 3.3ms preprocess, 11.1ms inference, 2.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 34 persons, 8.9ms\n",
            "Speed: 6.7ms preprocess, 8.9ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 37 persons, 1 dog, 10.1ms\n",
            "Speed: 3.7ms preprocess, 10.1ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 37 persons, 11.9ms\n",
            "Speed: 2.9ms preprocess, 11.9ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 36 persons, 1 backpack, 9.2ms\n",
            "Speed: 3.2ms preprocess, 9.2ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 37 persons, 1 backpack, 1 suitcase, 8.3ms\n",
            "Speed: 3.0ms preprocess, 8.3ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 35 persons, 1 backpack, 12.3ms\n",
            "Speed: 3.7ms preprocess, 12.3ms inference, 2.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 34 persons, 1 backpack, 13.4ms\n",
            "Speed: 4.4ms preprocess, 13.4ms inference, 2.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 34 persons, 1 backpack, 10.1ms\n",
            "Speed: 3.0ms preprocess, 10.1ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 35 persons, 1 dog, 1 backpack, 10.3ms\n",
            "Speed: 3.7ms preprocess, 10.3ms inference, 2.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 32 persons, 1 dog, 1 backpack, 9.2ms\n",
            "Speed: 3.5ms preprocess, 9.2ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 32 persons, 1 backpack, 11.5ms\n",
            "Speed: 3.5ms preprocess, 11.5ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 32 persons, 1 backpack, 10.0ms\n",
            "Speed: 3.7ms preprocess, 10.0ms inference, 2.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 31 persons, 1 dog, 1 backpack, 10.6ms\n",
            "Speed: 3.0ms preprocess, 10.6ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 32 persons, 1 backpack, 10.5ms\n",
            "Speed: 2.9ms preprocess, 10.5ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 35 persons, 1 backpack, 14.4ms\n",
            "Speed: 3.0ms preprocess, 14.4ms inference, 2.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 35 persons, 1 dog, 1 backpack, 12.9ms\n",
            "Speed: 11.4ms preprocess, 12.9ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 34 persons, 10.5ms\n",
            "Speed: 3.4ms preprocess, 10.5ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 34 persons, 1 backpack, 8.2ms\n",
            "Speed: 2.7ms preprocess, 8.2ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 32 persons, 10.5ms\n",
            "Speed: 4.3ms preprocess, 10.5ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 32 persons, 14.0ms\n",
            "Speed: 3.3ms preprocess, 14.0ms inference, 2.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 33 persons, 1 backpack, 2 handbags, 16.3ms\n",
            "Speed: 5.1ms preprocess, 16.3ms inference, 2.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 33 persons, 1 handbag, 10.5ms\n",
            "Speed: 3.0ms preprocess, 10.5ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 32 persons, 1 dog, 10.8ms\n",
            "Speed: 2.8ms preprocess, 10.8ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 35 persons, 2 dogs, 10.6ms\n",
            "Speed: 3.0ms preprocess, 10.6ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 33 persons, 1 dog, 10.4ms\n",
            "Speed: 3.5ms preprocess, 10.4ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 34 persons, 1 dog, 17.0ms\n",
            "Speed: 3.1ms preprocess, 17.0ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 35 persons, 1 handbag, 11.2ms\n",
            "Speed: 3.7ms preprocess, 11.2ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 34 persons, 2 dogs, 8.7ms\n",
            "Speed: 3.7ms preprocess, 8.7ms inference, 4.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 34 persons, 8.5ms\n",
            "Speed: 3.1ms preprocess, 8.5ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 32 persons, 9.6ms\n",
            "Speed: 3.0ms preprocess, 9.6ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 35 persons, 1 dog, 10.9ms\n",
            "Speed: 4.8ms preprocess, 10.9ms inference, 2.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 33 persons, 1 dog, 10.3ms\n",
            "Speed: 3.5ms preprocess, 10.3ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 32 persons, 1 dog, 8.4ms\n",
            "Speed: 3.2ms preprocess, 8.4ms inference, 2.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 31 persons, 1 dog, 1 backpack, 1 handbag, 14.6ms\n",
            "Speed: 3.1ms preprocess, 14.6ms inference, 2.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 28 persons, 1 backpack, 13.5ms\n",
            "Speed: 3.7ms preprocess, 13.5ms inference, 2.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 31 persons, 2 dogs, 1 backpack, 10.1ms\n",
            "Speed: 2.9ms preprocess, 10.1ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 30 persons, 1 dog, 8.8ms\n",
            "Speed: 3.1ms preprocess, 8.8ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 29 persons, 1 dog, 11.6ms\n",
            "Speed: 7.0ms preprocess, 11.6ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 26 persons, 1 dog, 1 handbag, 10.8ms\n",
            "Speed: 3.1ms preprocess, 10.8ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 26 persons, 1 dog, 1 handbag, 10.8ms\n",
            "Speed: 3.7ms preprocess, 10.8ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 29 persons, 2 dogs, 1 handbag, 14.1ms\n",
            "Speed: 3.6ms preprocess, 14.1ms inference, 2.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 28 persons, 1 dog, 11.7ms\n",
            "Speed: 4.0ms preprocess, 11.7ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 29 persons, 1 dog, 11.1ms\n",
            "Speed: 4.1ms preprocess, 11.1ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 27 persons, 10.4ms\n",
            "Speed: 2.8ms preprocess, 10.4ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 25 persons, 2 dogs, 11.6ms\n",
            "Speed: 2.9ms preprocess, 11.6ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 27 persons, 13.7ms\n",
            "Speed: 3.3ms preprocess, 13.7ms inference, 2.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 30 persons, 11.1ms\n",
            "Speed: 3.1ms preprocess, 11.1ms inference, 2.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 27 persons, 1 dog, 1 backpack, 13.3ms\n",
            "Speed: 2.8ms preprocess, 13.3ms inference, 2.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 28 persons, 16.3ms\n",
            "Speed: 2.9ms preprocess, 16.3ms inference, 3.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 29 persons, 21.1ms\n",
            "Speed: 4.9ms preprocess, 21.1ms inference, 2.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 28 persons, 12.3ms\n",
            "Speed: 4.6ms preprocess, 12.3ms inference, 2.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 30 persons, 2 handbags, 8.8ms\n",
            "Speed: 3.2ms preprocess, 8.8ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 29 persons, 1 handbag, 8.8ms\n",
            "Speed: 3.1ms preprocess, 8.8ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 28 persons, 12.4ms\n",
            "Speed: 3.0ms preprocess, 12.4ms inference, 2.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 28 persons, 1 bird, 1 handbag, 12.2ms\n",
            "Speed: 2.9ms preprocess, 12.2ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 30 persons, 1 dog, 9.6ms\n",
            "Speed: 3.1ms preprocess, 9.6ms inference, 2.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 30 persons, 10.2ms\n",
            "Speed: 3.2ms preprocess, 10.2ms inference, 2.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 29 persons, 11.9ms\n",
            "Speed: 3.9ms preprocess, 11.9ms inference, 2.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 28 persons, 1 bird, 11.3ms\n",
            "Speed: 3.8ms preprocess, 11.3ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 29 persons, 1 bird, 1 dog, 10.7ms\n",
            "Speed: 3.5ms preprocess, 10.7ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 27 persons, 1 dog, 1 handbag, 13.9ms\n",
            "Speed: 4.5ms preprocess, 13.9ms inference, 2.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 30 persons, 1 dog, 15.0ms\n",
            "Speed: 3.2ms preprocess, 15.0ms inference, 2.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 30 persons, 1 dog, 15.7ms\n",
            "Speed: 4.4ms preprocess, 15.7ms inference, 2.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 30 persons, 1 dog, 14.0ms\n",
            "Speed: 3.1ms preprocess, 14.0ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 29 persons, 1 dog, 8.3ms\n",
            "Speed: 3.1ms preprocess, 8.3ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 26 persons, 1 dog, 10.9ms\n",
            "Speed: 3.2ms preprocess, 10.9ms inference, 2.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 28 persons, 1 backpack, 1 handbag, 11.6ms\n",
            "Speed: 3.3ms preprocess, 11.6ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 30 persons, 11.4ms\n",
            "Speed: 3.0ms preprocess, 11.4ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 30 persons, 11.0ms\n",
            "Speed: 2.9ms preprocess, 11.0ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 32 persons, 11.4ms\n",
            "Speed: 3.3ms preprocess, 11.4ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 30 persons, 1 dog, 8.9ms\n",
            "Speed: 3.1ms preprocess, 8.9ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 35 persons, 10.9ms\n",
            "Speed: 3.0ms preprocess, 10.9ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 32 persons, 1 backpack, 1 handbag, 10.3ms\n",
            "Speed: 2.8ms preprocess, 10.3ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 31 persons, 1 backpack, 1 handbag, 14.3ms\n",
            "Speed: 3.3ms preprocess, 14.3ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 29 persons, 2 handbags, 11.1ms\n",
            "Speed: 2.9ms preprocess, 11.1ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 29 persons, 1 dog, 1 handbag, 16.1ms\n",
            "Speed: 3.1ms preprocess, 16.1ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 28 persons, 1 handbag, 12.1ms\n",
            "Speed: 4.7ms preprocess, 12.1ms inference, 2.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 29 persons, 1 handbag, 14.7ms\n",
            "Speed: 3.2ms preprocess, 14.7ms inference, 3.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 30 persons, 9.6ms\n",
            "Speed: 3.3ms preprocess, 9.6ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 29 persons, 8.9ms\n",
            "Speed: 3.4ms preprocess, 8.9ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 27 persons, 10.6ms\n",
            "Speed: 3.2ms preprocess, 10.6ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 28 persons, 11.2ms\n",
            "Speed: 3.1ms preprocess, 11.2ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 28 persons, 11.7ms\n",
            "Speed: 3.1ms preprocess, 11.7ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 30 persons, 10.9ms\n",
            "Speed: 3.3ms preprocess, 10.9ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 29 persons, 11.8ms\n",
            "Speed: 3.3ms preprocess, 11.8ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 30 persons, 11.3ms\n",
            "Speed: 4.0ms preprocess, 11.3ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 28 persons, 1 dog, 1 backpack, 10.6ms\n",
            "Speed: 3.2ms preprocess, 10.6ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 29 persons, 1 dog, 1 backpack, 10.2ms\n",
            "Speed: 3.8ms preprocess, 10.2ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 27 persons, 1 backpack, 10.0ms\n",
            "Speed: 4.9ms preprocess, 10.0ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 29 persons, 10.8ms\n",
            "Speed: 3.4ms preprocess, 10.8ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 31 persons, 10.8ms\n",
            "Speed: 3.2ms preprocess, 10.8ms inference, 2.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 29 persons, 1 bird, 11.9ms\n",
            "Speed: 4.4ms preprocess, 11.9ms inference, 2.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 27 persons, 1 bird, 2 dogs, 12.8ms\n",
            "Speed: 3.8ms preprocess, 12.8ms inference, 2.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 28 persons, 1 dog, 8.8ms\n",
            "Speed: 3.1ms preprocess, 8.8ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 25 persons, 2 dogs, 8.9ms\n",
            "Speed: 3.9ms preprocess, 8.9ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 25 persons, 2 dogs, 10.3ms\n",
            "Speed: 3.2ms preprocess, 10.3ms inference, 2.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 30 persons, 1 dog, 10.8ms\n",
            "Speed: 3.2ms preprocess, 10.8ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 26 persons, 1 dog, 1 suitcase, 8.8ms\n",
            "Speed: 6.9ms preprocess, 8.8ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 28 persons, 2 dogs, 12.5ms\n",
            "Speed: 3.1ms preprocess, 12.5ms inference, 2.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 29 persons, 2 dogs, 8.0ms\n",
            "Speed: 2.8ms preprocess, 8.0ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 29 persons, 1 bird, 12.1ms\n",
            "Speed: 5.4ms preprocess, 12.1ms inference, 2.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 30 persons, 1 dog, 21.0ms\n",
            "Speed: 3.1ms preprocess, 21.0ms inference, 2.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 30 persons, 1 dog, 9.2ms\n",
            "Speed: 4.9ms preprocess, 9.2ms inference, 2.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 30 persons, 23.1ms\n",
            "Speed: 5.2ms preprocess, 23.1ms inference, 3.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 33 persons, 1 dog, 19.0ms\n",
            "Speed: 4.9ms preprocess, 19.0ms inference, 4.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 29 persons, 1 dog, 11.8ms\n",
            "Speed: 3.1ms preprocess, 11.8ms inference, 3.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 31 persons, 1 backpack, 17.7ms\n",
            "Speed: 3.2ms preprocess, 17.7ms inference, 2.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 31 persons, 1 backpack, 11.0ms\n",
            "Speed: 3.1ms preprocess, 11.0ms inference, 3.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 27 persons, 1 dog, 1 backpack, 11.6ms\n",
            "Speed: 3.2ms preprocess, 11.6ms inference, 2.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 26 persons, 1 dog, 1 backpack, 12.6ms\n",
            "Speed: 3.1ms preprocess, 12.6ms inference, 2.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 28 persons, 1 dog, 2 backpacks, 1 suitcase, 11.1ms\n",
            "Speed: 3.1ms preprocess, 11.1ms inference, 2.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 27 persons, 1 dog, 1 backpack, 11.3ms\n",
            "Speed: 3.2ms preprocess, 11.3ms inference, 2.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 27 persons, 1 dog, 14.5ms\n",
            "Speed: 3.2ms preprocess, 14.5ms inference, 2.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 28 persons, 1 dog, 18.2ms\n",
            "Speed: 5.0ms preprocess, 18.2ms inference, 2.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 28 persons, 1 dog, 16.6ms\n",
            "Speed: 3.4ms preprocess, 16.6ms inference, 4.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 28 persons, 1 dog, 17.3ms\n",
            "Speed: 4.8ms preprocess, 17.3ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 26 persons, 1 dog, 12.3ms\n",
            "Speed: 4.3ms preprocess, 12.3ms inference, 2.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 25 persons, 2 dogs, 11.1ms\n",
            "Speed: 3.2ms preprocess, 11.1ms inference, 2.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 29 persons, 1 dog, 15.3ms\n",
            "Speed: 3.1ms preprocess, 15.3ms inference, 2.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 28 persons, 2 dogs, 11.9ms\n",
            "Speed: 3.2ms preprocess, 11.9ms inference, 2.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 27 persons, 1 dog, 1 backpack, 16.8ms\n",
            "Speed: 3.9ms preprocess, 16.8ms inference, 2.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 25 persons, 1 dog, 1 backpack, 16.9ms\n",
            "Speed: 3.1ms preprocess, 16.9ms inference, 2.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 30 persons, 1 dog, 1 backpack, 14.6ms\n",
            "Speed: 5.0ms preprocess, 14.6ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 28 persons, 2 dogs, 11.8ms\n",
            "Speed: 3.1ms preprocess, 11.8ms inference, 2.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 29 persons, 22.8ms\n",
            "Speed: 9.3ms preprocess, 22.8ms inference, 2.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 29 persons, 13.0ms\n",
            "Speed: 3.6ms preprocess, 13.0ms inference, 6.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 29 persons, 1 backpack, 14.0ms\n",
            "Speed: 3.4ms preprocess, 14.0ms inference, 2.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 29 persons, 13.9ms\n",
            "Speed: 3.2ms preprocess, 13.9ms inference, 2.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 28 persons, 1 backpack, 14.9ms\n",
            "Speed: 6.3ms preprocess, 14.9ms inference, 4.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 27 persons, 10.8ms\n",
            "Speed: 7.2ms preprocess, 10.8ms inference, 2.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 26 persons, 14.9ms\n",
            "Speed: 8.5ms preprocess, 14.9ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 28 persons, 10.6ms\n",
            "Speed: 3.7ms preprocess, 10.6ms inference, 2.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 29 persons, 15.3ms\n",
            "Speed: 4.4ms preprocess, 15.3ms inference, 2.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 28 persons, 15.3ms\n",
            "Speed: 3.2ms preprocess, 15.3ms inference, 2.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 28 persons, 16.8ms\n",
            "Speed: 4.7ms preprocess, 16.8ms inference, 2.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 29 persons, 11.9ms\n",
            "Speed: 3.7ms preprocess, 11.9ms inference, 2.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 27 persons, 14.3ms\n",
            "Speed: 5.0ms preprocess, 14.3ms inference, 2.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 28 persons, 15.2ms\n",
            "Speed: 3.2ms preprocess, 15.2ms inference, 3.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 28 persons, 13.7ms\n",
            "Speed: 4.6ms preprocess, 13.7ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Video frame is empty or video processing has been successfully completed.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Instance seg+ obj track(mask filled with color ) will be released 8 Sep , 2024"
      ],
      "metadata": {
        "id": "1Yd-CcASbAnp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# This part of code snippet will be updated later on"
      ],
      "metadata": {
        "id": "OV1fimpYGKWP"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}